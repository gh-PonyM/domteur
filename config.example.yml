# Example config to run outside of docker to use GPU
version: "0.1.0"

# LLM Provider Configuration
llm_providers:
  # Local Ollama instance
  - name: llama2
    type: "ollama"
    base_url: &ollama-local "http://ollama:11434"
    model: llama2
    system_prompt: &prompt |
      You are a helpful AI Assistant. Keep conversations short if not otherwise told.
      You response it sent to a text-to-speech engine. Format your response so that the text can be properly read. 
      Do not use bullet points. Use words such as "first", "second" instead of creating "1.", "2.". 
      Your response should be like the text in a book.
  - name: gpt-oss
    type: "ollama"
    base_url: *ollama-local
    model: gpt-oss:20b
    system_prompt: *prompt

  # Alternative Ollama models
  # - type: "ollama"
  #   base_url: "http://localhost:11434" 
  #   model: "mistral"
  
  # OpenRouter cloud provider
  # - type: "openrouter"
  #   api_key: "sk-or-v1-xxxxx"  # Set via environment variable OPENROUTER_API_KEY
  #   model: "anthropic/claude-3-haiku"
  #   base_url: "https://openrouter.ai/api/v1"

# Database Configuration  
database:
  type: "sqlite"
  path: "./domteur.db"

# Text-to-Speech Configuration
tts:
  engine: "piper"
  voice_model_name: en_US-norman-medium
  volume: 1.0
  speed: 1.3
  use_cuda: false
  auto_download_voice: true
  sample_rate: 22050
  chunk_size: 1024

# Start the broker separately using docker compose up broker
broker_host: "broker"
# Speach to Text Settings Whisper engine
sst:
  beam_size: 5
  compute_type: int8
  device: cpu
  model_size: large-v3
  word_timestamps: true
