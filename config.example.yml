# Example config to run outside of docker to use GPU
version: "0.1.0"

# LLM Provider Configuration
llm_providers:
  # Local Ollama instance
  - type: "ollama"
    base_url: "http://localhost:11434"
    model: "llama2"
    system_prompt: |
      You are a helpful AI Assistant. Keep conversations short if not otherwise told.
  
  # Alternative Ollama models
  # - type: "ollama"
  #   base_url: "http://localhost:11434" 
  #   model: "mistral"
  
  # OpenRouter cloud provider
  # - type: "openrouter"
  #   api_key: "sk-or-v1-xxxxx"  # Set via environment variable OPENROUTER_API_KEY
  #   model: "anthropic/claude-3-haiku"
  #   base_url: "https://openrouter.ai/api/v1"

# Database Configuration  
database:
  type: "sqlite"
  path: "./domteur.db"

# Text-to-Speech Configuration
tts:
  engine: "piper"
  voice_model_name: en_US-bryce-medium
#  voice_config_path: "en_US-lessac-medium.onnx.json"
  volume: 1.0
  speed: 1.0
  use_cuda: false
  auto_download_voice: true
  sample_rate: 22050
  chunk_size: 1024

# Start the broker separately using docker compose up broker
broker_host: "localhost"
